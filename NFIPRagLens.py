# -*- coding: utf-8 -*-
"""ragpipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lrQ4HE06SvLW9DTgn8VkHcSo-pG3aDcK
"""

# ```python
# !pip install -U langchain langchain-community sentence-transformers transformers accelerate faiss-cpu pypdf streamlit langchain-huggingface langchain-core
# ```

import requirements

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from transformers import pipeline

from transformers import AutoModel
from sentence_transformers import SentenceTransformer

SentenceTransformer("intfloat/e5-small-v2")
print("OK: embeddings load")

import requests

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_huggingface import HuggingFacePipeline

PDF_URL = "https://www.fema.gov/sites/default/files/documents/fema_nfip_flood-insurance-manual_102025.pdf"
PDF_PATH = "nfip_manual.pdf"

def download_pdf():
    if not os.path.exists(PDF_PATH):
        response = requests.get(PDF_URL)
        with open(PDF_PATH, "wb") as f:
            f.write(response.content)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)[:1500]

def build_rag_chain():
    # Download PDF
    download_pdf()

    # Load PDF
    loader = PyPDFLoader(PDF_PATH)
    pages = loader.load()

    # Chunk
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " "]
    )
    docs = splitter.split_documents(pages)

    # Embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="intfloat/e5-small-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )

    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # LLM
    hf_pipeline = pipeline(
        "text2text-generation",
        model="google/flan-t5-base",
        max_new_tokens=256
    )
    llm = HuggingFacePipeline(pipeline=hf_pipeline)

    # Prompt
    prompt = PromptTemplate.from_template("""
    You are an assistant answering questions about FEMA's NFIP Flood Insurance Manual.

    Use ONLY the provided context.
    List ALL applicable items.
    Do NOT summarize or omit items.

    Context:
    {context}

    Question:
    {question}



    Answer (bulleted list):
    """)

    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
    )

    return rag_chain

RAG_CHAIN = build_rag_chain()

def ask_question(question: str) -> str:
    """Call this from Streamlit"""
    return RAG_CHAIN.invoke(question)
